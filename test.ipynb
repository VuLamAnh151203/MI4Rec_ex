{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import json\n",
                "import pickle\n",
                "import numpy as np\n",
                "from typing import Any\n",
                "import builtins\n",
                "from unittest.mock import MagicMock, patch\n",
                "from torch import nn\n",
                "from scipy.sparse import load_npz\n",
                "\n",
                "# Add src to sys.path\n",
                "current_dir = os.getcwd()\n",
                "src_path = os.path.join(current_dir, 'src')\n",
                "if src_path not in sys.path:\n",
                "    sys.path.append(src_path)\n",
                "\n",
                "# Imports from src\n",
                "from model import (\n",
                "    UserItemMemory,\n",
                "    DynamicDPELLM4RecBaseModel,\n",
                "    DynamicCollaborativeGPTwithItemRecommendHead,\n",
                "    MSEDynamicDPELLM4RecBaseModel,\n",
                "    EmbeddingMapper\n",
                ")\n",
                "from tokenizer import DynamicBPETokenizerBatch\n",
                "from transformers import GPT2Config, AutoModel, AutoModelForSequenceClassification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration and Paths\n",
                "root_dir = current_dir\n",
                "dataset = \"Beauty\"\n",
                "# Path to your weight file\n",
                "rec_model_path = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\model\\Beauty\\rec\\rec_model_0.1_300_0.2_stella_0.001.pth\"\n",
                "data_root = os.path.join(root_dir, \"dataset\", dataset)\n",
                "model_name = \"gpt2\" # Assumed base model\n",
                "\n",
                "# Parameters inferred from filename / user request\n",
                "lambda_V = 0.1\n",
                "num_meta = 300\n",
                "cold_start = 0.2\n",
                "item_logits_infer = \"stella\"\n",
                "lr = 0.001\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load Data to get shapes\n",
                "train_mat_path = os.path.join(data_root, \"warm_train_matrix.npz\")\n",
                "if not os.path.exists(train_mat_path):\n",
                "    # Try alternative path if structure is different\n",
                "    train_mat_path = os.path.join(data_root, \"0.8\", \"warm_train_matrix.npz\") # 1-0.2=0.8\n",
                "\n",
                "print(f\"Loading train matrix from: {train_mat_path}\")\n",
                "train_mat = load_npz(train_mat_path)\n",
                "num_users, num_items = train_mat.shape\n",
                "print(f\"Num Users: {num_users}, Num Items: {num_items}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Inspect Checkpoint to get Missing Dimensions (for stella embeddings)\n",
                "print(f\"Loading checkpoint state dict from {rec_model_path}...\")\n",
                "state_dict = torch.load(rec_model_path, map_location=device)\n",
                "\n",
                "# We need to know the shape of 'base_model.item_src_embs.weight' to create a dummy initialization\n",
                "item_src_embs_weight = state_dict.get('base_model.item_src_embs.weight')\n",
                "if item_src_embs_weight is not None:\n",
                "    stella_num_items, stella_emb_dim = item_src_embs_weight.shape\n",
                "    print(f\"Found item_src_embs in checkpoint: shape=({stella_num_items}, {stella_emb_dim})\")\n",
                "else:\n",
                "    print(\"WARNING: item_src_embs.weight not found in checkpoint!\")\n",
                "    stella_emb_dim = 1024 # Fallback guess or check error\n",
                "    stella_num_items = num_items"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Mock pickle.load to bypass missing file '/shared/user/embs/...'\n",
                "# The model init calls: pickle.load(open(..., 'rb'))\n",
                "# We will mock 'open' and 'pickle.load' specifically for this call.\n",
                "\n",
                "original_open = builtins.open\n",
                "original_pickle_load = pickle.load\n",
                "\n",
                "class MockFile:\n",
                "    def __enter__(self): return self\n",
                "    def __exit__(self, *args): pass\n",
                "    def read(self): return b\"\"\n",
                "\n",
                "def mocked_open(file, mode='r', *args, **kwargs):\n",
                "    if \"item_review_embeddings.pkl\" in str(file):\n",
                "        print(f\"Intercepted open for: {file}\")\n",
                "        return MockFile()\n",
                "    return original_open(file, mode, *args, **kwargs)\n",
                "\n",
                "def mocked_pickle_load(file_obj, *args, **kwargs):\n",
                "    if isinstance(file_obj, MockFile):\n",
                "        print(\"Intercepted pickle.load, returning dummy embeddings\")\n",
                "        # Return dummy numpy array with correct shape\n",
                "        return np.random.rand(stella_num_items, stella_emb_dim).astype(np.float32)\n",
                "    return original_pickle_load(file_obj, *args, **kwargs)\n",
                "\n",
                "# Activate mocks\n",
                "builtins.open = mocked_open\n",
                "pickle.load = mocked_pickle_load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Initialize Model components\n",
                "\n",
                "# Config\n",
                "config = GPT2Config.from_pretrained('gpt2')\n",
                "config.num_users = num_users\n",
                "config.num_items = num_items\n",
                "config.n_embd = config.hidden_size\n",
                "\n",
                "# LLM Model (GPT2)\n",
                "# Assuming internet access or cached model. \n",
                "# If offline, point to: os.path.join(root_dir, \"model\", \"pretrained\", \"gpt2\")\n",
                "llm_model_path = 'gpt2'\n",
                "local_gpt2 = os.path.join(root_dir, \"model\", \"pretrained\", \"gpt2\")\n",
                "if os.path.exists(local_gpt2):\n",
                "    llm_model_path = local_gpt2\n",
                "\n",
                "print(f\"Loading GPT2 from: {llm_model_path}\")\n",
                "LLMmodel = AutoModel.from_pretrained(llm_model_path)\n",
                "\n",
                "# Memory (Empty is fine for weight loading, but class needs it)\n",
                "memory = UserItemMemory()\n",
                "\n",
                "# Base Classifier (BERT)\n",
                "# Used for 'classifier' mode, but constructor might require it even for 'stella' if not careful.\n",
                "# The code in model.py init:\n",
                "# self.meta_logits_classifier = meta_logits_classifier.to(device) (unconditional assignment)\n",
                "# So we need to pass something valid or a mock.\n",
                "# Since we use 'stella', logic later: if item_logits_infer == 'stella': ...\n",
                "# It doesn't use meta_logits_classifier in 'stella' mode initialization logic (lines 201-206).\n",
                "# But it is assigned to self.meta_logits_classifier.\n",
                "# We will pass a dummy mock to avoid loading BERT.\n",
                "dummy_classifier = MagicMock()\n",
                "dummy_classifier.to.return_value = dummy_classifier\n",
                "\n",
                "print(\"Initializing MSEDynamicDPELLM4RecBaseModel...\")\n",
                "base_model = MSEDynamicDPELLM4RecBaseModel(\n",
                "    config,\n",
                "    LLMmodel,\n",
                "    memory,\n",
                "    meta_logits_tokenizer=None,\n",
                "    meta_logits_classifier=dummy_classifier,\n",
                "    device=device,\n",
                "    num_item_meta=num_meta,\n",
                "    item_logits_infer=item_logits_infer,\n",
                "    dataset_name=dataset\n",
                ")\n",
                "\n",
                "# Wrap in Recommend Head\n",
                "print(\"Initializing DynamicCollaborativeGPTwithItemRecommendHead...\")\n",
                "rec_model = DynamicCollaborativeGPTwithItemRecommendHead(config, base_model, device=device)\n",
                "\n",
                "# Restore original open/pickle\n",
                "builtins.open = original_open\n",
                "pickle.load = original_pickle_load\n",
                "print(\"Mocks removed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Load Weights\n",
                "print(\"Loading state dict into model...\")\n",
                "rec_model.load_state_dict(state_dict, strict=False)\n",
                "rec_model.eval()\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Extract Components\n",
                "# - user embedding\n",
                "# - item embedding (base_model.item_src_embs)\n",
                "# - token-meta embedding (base_model.meta_item_embeddings)\n",
                "# - MLP to compute logits (base_model.item_emb_mapper)\n",
                "\n",
                "user_embeddings = rec_model.base_model.user_embeddings.weight.data\n",
                "item_embeddings = rec_model.base_model.item_src_embs.weight.data\n",
                "token_meta_embeddings = rec_model.base_model.meta_item_embeddings.weight.data\n",
                "mlp_model = rec_model.base_model.item_emb_mapper\n",
                "\n",
                "print(\"\\n--- Extracted Components ---\")\n",
                "print(f\"User Embeddings Shape: {user_embeddings.shape}\")\n",
                "print(f\"Item Embeddings (Source/Stella) Shape: {item_embeddings.shape}\")\n",
                "print(f\"Token-Meta Embeddings Shape: {token_meta_embeddings.shape}\")\n",
                "print(f\"MLP Model: {mlp_model}\")\n",
                "\n",
                "# Verify MLP Computation\n",
                "# logits = mlp(item_embedding) -> scaled to meta size\n",
                "# weighted_emb = probs(logits) @ meta_embeddings\n",
                "\n",
                "with torch.no_grad():\n",
                "    test_item_emb = item_embeddings[0].unsqueeze(0)\n",
                "    logits = mlp_model(test_item_emb)\n",
                "    print(f\"Test MLP Logits Shape: {logits.shape}\")  # Should be (1, num_meta)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Content-Based Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import scipy.sparse as sp\n",
                "\n",
                "# File Paths defined by User\n",
                "TEST_MATRIX_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\cold_item_test_matrix.npz\"\n",
                "ITEM_EMBS_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\Beauty_item_review_embeddings_full.pkl\"\n",
                "WARM_TRAIN_MATRIX_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\warm_train_matrix.npz\"\n",
                "\n",
                "print(\"--- Loading Evaluation Datasets ---\")\n",
                "\n",
                "# 1. Load Ground Truth (Cold Item Test Matrix)\n",
                "print(f\"Loading Test Matrix: {TEST_MATRIX_PATH}\")\n",
                "test_matrix = load_npz(TEST_MATRIX_PATH)\n",
                "print(f\"Test Matrix Shape (Users x Items): {test_matrix.shape}\")\n",
                "\n",
                "# 2. Load Item Embeddings\n",
                "print(f\"Loading Item Embeddings: {ITEM_EMBS_PATH}\")\n",
                "with open(ITEM_EMBS_PATH, \"rb\") as f:\n",
                "    # These are likely numpy arrays of shape (NumItems, EmbDim)\n",
                "    all_item_embeddings = pickle.load(f)\n",
                "print(f\"All Item Embeddings Shape: {all_item_embeddings.shape}\")\n",
                "\n",
                "# 3. Load User History (Warm Train Matrix)\n",
                "print(f\"Loading Warm Train Matrix: {WARM_TRAIN_MATRIX_PATH}\")\n",
                "warm_train_matrix = load_npz(WARM_TRAIN_MATRIX_PATH)\n",
                "print(f\"Warm Train Matrix Shape: {warm_train_matrix.shape}\")\n",
                "num_users_train, num_items_train = warm_train_matrix.shape\n",
                "\n",
                "# --- Evaluation Logic ---\n",
                "\n",
                "def calculate_recall_at_k(predictions, ground_truth, k=20):\n",
                "    \"\"\"\n",
                "    predictions: list of lists or 2D array, top K item indices for each user\n",
                "    ground_truth: list of lists or 2D sparse row, ground truth item indices for each user\n",
                "    \"\"\"\n",
                "    recalls = []\n",
                "    \n",
                "    # Convert sparse matrix to list of lists for easier iteration if needed, \n",
                "    # or iterate rows efficiently.\n",
                "    \n",
                "    for user_idx in range(len(predictions)):\n",
                "        pred_items = set(predictions[user_idx][:k])\n",
                "        \n",
                "        # Handle sparse matrix ground truth\n",
                "        if sp.issparse(ground_truth):\n",
                "            true_items = set(ground_truth[user_idx].indices)\n",
                "        else:\n",
                "            true_items = set(ground_truth[user_idx])\n",
                "        \n",
                "        if len(true_items) == 0:\n",
                "            continue\n",
                "            \n",
                "        hits = len(pred_items & true_items)\n",
                "        recalls.append(hits / len(true_items))\n",
                "        \n",
                "    return np.mean(recalls)\n",
                "\n",
                "print(\"\\n--- Computing User Embeddings and Evaluating ---\")\n",
                "\n",
                "# We only evaluate users who exist in the test matrix row-space.\n",
                "# Assuming the row indices in test_matrix correspond to the same users in warm_train_matrix.\n",
                "# (Usually indices are consistent 0..N-1).\n",
                "\n",
                "batch_size = 100\n",
                "num_test_users = test_matrix.shape[0]\n",
                "k = 20\n",
                "\n",
                "all_recalls = []\n",
                "\n",
                "# Pre-normalize item embeddings for fast cosine similarity\n",
                "# Cosine Sim(A, B) = Dot(Norm(A), Norm(B))\n",
                "norm_item_embeddings = all_item_embeddings / np.linalg.norm(all_item_embeddings, axis=1, keepdims=True)\n",
                "norm_item_embeddings = np.nan_to_num(norm_item_embeddings) # Handle potential zero vectors\n",
                "\n",
                "for start_idx in range(0, num_test_users, batch_size):\n",
                "    end_idx = min(start_idx + batch_size, num_test_users)\n",
                "    user_indices = np.arange(start_idx, end_idx)\n",
                "    \n",
                "    # Get history for this batch of users\n",
                "    # user_history_batch is (BatchSize, NumItems), sparse\n",
                "    user_history_batch = warm_train_matrix[user_indices]\n",
                "    \n",
                "    batch_user_embeddings = []\n",
                "    \n",
                "    # For each user in batch, average the embeddings of their interacted items\n",
                "    # Optimized: Batch Matrix Multiplication is possible if we treat sparse matrix as weights\n",
                "    # UserEmb = (HistoryMatrix @ ItemMatrix) / Count\n",
                "    \n",
                "    # 1. Sum of item embeddings\n",
                "    # Shape: (BatchSize, NumItems) * (NumItems, EmbDim) -> (BatchSize, EmbDim)\n",
                "    batch_user_sums = user_history_batch @ all_item_embeddings\n",
                "    \n",
                "    # 2. Counts (interactions per user)\n",
                "    user_counts = np.array(user_history_batch.sum(axis=1)).flatten()\n",
                "    user_counts[user_counts == 0] = 1 # Avoid division by zero\n",
                "    \n",
                "    # 3. Average\n",
                "    batch_user_embs = batch_user_sums / user_counts[:, np.newaxis]\n",
                "    \n",
                "    # Normalize user embeddings\n",
                "    batch_user_embs_norm = batch_user_embs / np.linalg.norm(batch_user_embs, axis=1, keepdims=True)\n",
                "    batch_user_embs_norm = np.nan_to_num(batch_user_embs_norm)\n",
                "    \n",
                "    # Compute Similarity scores: (BatchSize, EmbDim) @ (EmbDim, NumItems) -> (BatchSize, NumItems)\n",
                "    scores = batch_user_embs_norm @ norm_item_embeddings.T\n",
                "    \n",
                "    # Masking: We usually want to recommend COLD items, or ALL items?\n",
                "    # The request implies \"cosine similarity with all item embedding\".\n",
                "    # Typically in RecSys, we mask out training items, but if targets are strictly cold items\n",
                "    # and we look at all items, the cold items should bubble up if relevant.\n",
                "    # However, to be strict, we might want to mask items already seen in training.\n",
                "    # (Optional: user_history_batch > 0 -> -inf)\n",
                "    # Since we are evaluating recall on specific test items, we usually just rank all and check hits.\n",
                "    \n",
                "    # Get Top-K indices\n",
                "    # argpartition is faster than sort for top-k\n",
                "    # We want indices of highest scores\n",
                "    top_k_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
                "    \n",
                "    # Sort within top k for exact ranking (though for Recall set intersection it doesn't matter, \n",
                "    # but argpartition doesn't guarantee order inside top k)\n",
                "    # For Normalized Recall@K, set intersection is enough.\n",
                "    \n",
                "    # Evaluate Batch\n",
                "    batch_recalls = []\n",
                "    test_interactions_batch = test_matrix[user_indices]\n",
                "    \n",
                "    for i in range(len(user_indices)):\n",
                "        pred_indices = top_k_indices[i]\n",
                "        true_indices = test_interactions_batch[i].indices\n",
                "        \n",
                "        if len(true_indices) > 0:\n",
                "            hits = np.intersect1d(pred_indices, true_indices).size\n",
                "            batch_recalls.append(hits / len(true_indices))\n",
                "            \n",
                "    all_recalls.extend(batch_recalls)\n",
                "\n",
                "final_recall_20 = np.mean(all_recalls)\n",
                "print(f\"\\nFinal Content-Based Recall@20: {final_recall_20:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refined Content-Based Evaluation (MLP + Meta-Item Embeddings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Computing Refined Item Embeddings ---\")\n",
                "\n",
                "# 1. Convert Raw Embeddings to Tensor\n",
                "raw_item_embs_tensor = torch.tensor(all_item_embeddings, dtype=torch.float32).to(device)\n",
                "\n",
                "# 2. Calculate New Content Embeddings via Model Pipeline\n",
                "\n",
                "# Ensure models are in evaluation mode\n",
                "mlp_model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    # A. Pass through MLP to get Logits (n_items, n_meta)\n",
                "    # mlp_model (EmbeddingMapper) is likely (EmbDim -> ... -> NumMeta)\n",
                "    item_logits = mlp_model(raw_item_embs_tensor)\n",
                "    print(f\"Item Logits Shape: {item_logits.shape}\")\n",
                "    \n",
                "    # B. Apply Softmax to get probabilities (Weights)\n",
                "    # Consistent with model.py get_probs(self, logits) with prob_norm='softmax'\n",
                "    item_probs = torch.nn.functional.softmax(item_logits, dim=-1)\n",
                "    \n",
                "    # C. Multiply with Meta-Item Embeddings\n",
                "    # token_meta_embeddings is a Tensor (NumMeta, EmbDim)\n",
                "    # weighted_item_embeddings = item_probs @ meta_item_embeddings\n",
                "    # Shape: (NumItems, NumMeta) @ (NumMeta, EmbDim) -> (NumItems, EmbDim)\n",
                "    if not isinstance(token_meta_embeddings, torch.Tensor):\n",
                "        token_meta_embeddings = torch.tensor(token_meta_embeddings).to(device)\n",
                "    \n",
                "    refined_item_embeddings = torch.matmul(item_probs, token_meta_embeddings)\n",
                "    print(f\"Refined Item Embeddings Shape: {refined_item_embeddings.shape}\")\n",
                "\n",
                "# Convert back to Numpy for evaluation loop\n",
                "refined_item_embeddings_np = refined_item_embeddings.cpu().numpy()\n",
                "\n",
                "print(\"--- Evaluating with Refined Embeddings ---\")\n",
                "\n",
                "# Reuse Evaluation Loop logic, but with refined_item_embeddings_np\n",
                "\n",
                "all_recalls_refined = []\n",
                "\n",
                "# Normalized Refined Item Embeddings\n",
                "norm_refined_item_embeddings = refined_item_embeddings_np / np.linalg.norm(refined_item_embeddings_np, axis=1, keepdims=True)\n",
                "norm_refined_item_embeddings = np.nan_to_num(norm_refined_item_embeddings)\n",
                "\n",
                "for start_idx in range(0, num_test_users, batch_size):\n",
                "    end_idx = min(start_idx + batch_size, num_test_users)\n",
                "    user_indices = np.arange(start_idx, end_idx)\n",
                "    \n",
                "    # Input User History is still the same warm interactions\n",
                "    user_history_batch = warm_train_matrix[user_indices]\n",
                "    \n",
                "    # Calculate User Embeddings using REFINED item embeddings\n",
                "    # BatchUserSums = History @ RefinedItemMatrix\n",
                "    batch_user_sums = user_history_batch @ refined_item_embeddings_np\n",
                "    \n",
                "    user_counts = np.array(user_history_batch.sum(axis=1)).flatten()\n",
                "    user_counts[user_counts == 0] = 1\n",
                "    \n",
                "    batch_user_embs = batch_user_sums / user_counts[:, np.newaxis]\n",
                "    \n",
                "    # Norm\n",
                "    batch_user_embs_norm = batch_user_embs / np.linalg.norm(batch_user_embs, axis=1, keepdims=True)\n",
                "    batch_user_embs_norm = np.nan_to_num(batch_user_embs_norm)\n",
                "    \n",
                "    # Scores: Users @ RefinedItems.T\n",
                "    scores = batch_user_embs_norm @ norm_refined_item_embeddings.T\n",
                "    \n",
                "    # Top K\n",
                "    top_k_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
                "    \n",
                "    # Evaluate against SAME ground truth\n",
                "    batch_recalls = []\n",
                "    test_interactions_batch = test_matrix[user_indices]\n",
                "    \n",
                "    for i in range(len(user_indices)):\n",
                "        pred_indices = top_k_indices[i]\n",
                "        true_indices = test_interactions_batch[i].indices\n",
                "        \n",
                "        if len(true_indices) > 0:\n",
                "            hits = np.intersect1d(pred_indices, true_indices).size\n",
                "            batch_recalls.append(hits / len(true_indices))\n",
                "            \n",
                "    all_recalls_refined.extend(batch_recalls)\n",
                "\n",
                "final_recall_20_refined = np.mean(all_recalls_refined)\n",
                "print(f\"\\nFinal Refined Content-Based Recall@20: {final_recall_20_refined:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Warm Test Data using Model Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "WARM_TEST_MATRIX_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\warm_test_matrix.npz\"\n",
                "\n",
                "print(\"--- Evaluating Warm Test Data with Model Learned Embeddings ---\")\n",
                "\n",
                "# 1. Load Warm Test Matrix\n",
                "print(f\"Loading Warm Test Matrix: {WARM_TEST_MATRIX_PATH}\")\n",
                "warm_test_matrix = load_npz(WARM_TEST_MATRIX_PATH)\n",
                "print(f\"Warm Test Matrix Shape: {warm_test_matrix.shape}\")\n",
                "\n",
                "# 2. Use Model Embeddings directly\n",
                "# user_embeddings: Learned User Embeddings from model (NumUsers, EmbDim) (Tensor)\n",
                "# refined_item_embeddings_np: Refined Item Embeddings (NumItems, EmbDim) (Numpy)\n",
                "\n",
                "if isinstance(user_embeddings, torch.Tensor):\n",
                "    user_embeddings_np = user_embeddings.cpu().numpy()\n",
                "else:\n",
                "    user_embeddings_np = user_embeddings\n",
                "\n",
                "# Normalize for Cosine Similarity\n",
                "user_embeddings_norm = user_embeddings_np / np.linalg.norm(user_embeddings_np, axis=1, keepdims=True)\n",
                "user_embeddings_norm = np.nan_to_num(user_embeddings_norm)\n",
                "\n",
                "norm_refined_item_embeddings = refined_item_embeddings_np / np.linalg.norm(refined_item_embeddings_np, axis=1, keepdims=True)\n",
                "norm_refined_item_embeddings = np.nan_to_num(norm_refined_item_embeddings)\n",
                "\n",
                "all_warm_recalls = []\n",
                "num_users = warm_test_matrix.shape[0]\n",
                "\n",
                "for start_idx in range(0, num_users, batch_size):\n",
                "    end_idx = min(start_idx + batch_size, num_users)\n",
                "    user_indices = np.arange(start_idx, end_idx)\n",
                "    \n",
                "    # Batch User Embeddings\n",
                "    batch_user_embs = user_embeddings_norm[user_indices]\n",
                "    \n",
                "    # Calculate Scores\n",
                "    scores = batch_user_embs @ norm_refined_item_embeddings.T\n",
                "    \n",
                "    # Top K\n",
                "    top_k_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
                "    \n",
                "    # Evaluation Match\n",
                "    warm_test_batch = warm_test_matrix[user_indices]\n",
                "    \n",
                "    batch_recalls = []\n",
                "    for i in range(len(user_indices)):\n",
                "        pred_indices = top_k_indices[i]\n",
                "        true_indices = warm_test_batch[i].indices\n",
                "        \n",
                "        if len(true_indices) > 0:\n",
                "            hits = np.intersect1d(pred_indices, true_indices).size\n",
                "            batch_recalls.append(hits / len(true_indices))\n",
                "            \n",
                "    all_warm_recalls.extend(batch_recalls)\n",
                "\n",
                "final_warm_recall_20 = np.mean(all_warm_recalls)\n",
                "print(f\"\\nFinal Warm Test Recall@20 (Model Embeddings): {final_warm_recall_20:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}