{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import torch\n",
                "import json\n",
                "import pickle\n",
                "import numpy as np\n",
                "from typing import Any\n",
                "import builtins\n",
                "from unittest.mock import MagicMock, patch\n",
                "from torch import nn\n",
                "from scipy.sparse import load_npz\n",
                "\n",
                "# Add src to sys.path\n",
                "current_dir = os.getcwd()\n",
                "src_path = os.path.join(current_dir, 'src')\n",
                "if src_path not in sys.path:\n",
                "    sys.path.append(src_path)\n",
                "\n",
                "# Imports from src\n",
                "from model import (\n",
                "    UserItemMemory,\n",
                "    DynamicDPELLM4RecBaseModel,\n",
                "    DynamicCollaborativeGPTwithItemRecommendHead,\n",
                "    MSEDynamicDPELLM4RecBaseModel,\n",
                "    EmbeddingMapper\n",
                ")\n",
                "from tokenizer import DynamicBPETokenizerBatch\n",
                "from transformers import GPT2Config, AutoModel, AutoModelForSequenceClassification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cuda\n"
                    ]
                }
            ],
            "source": [
                "# Configuration and Paths\n",
                "root_dir = current_dir\n",
                "dataset = \"Beauty\"\n",
                "# Path to your weight file\n",
                "rec_model_path = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\model\\Beauty\\rec\\rec_model_0.1_300_0.2_stella_0.001.pth\"\n",
                "data_root = os.path.join(root_dir, \"dataset\", dataset)\n",
                "model_name = \"gpt2\" # Assumed base model\n",
                "\n",
                "# Parameters inferred from filename / user request\n",
                "lambda_V = 0.1\n",
                "num_meta = 300\n",
                "cold_start = 0.2\n",
                "item_logits_infer = \"stella\"\n",
                "lr = 0.001\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading train matrix from: c:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\Beauty\\warm_train_matrix.npz\n",
                        "Num Users: 22363, Num Items: 12101\n"
                    ]
                }
            ],
            "source": [
                "# 1. Load Data to get shapes\n",
                "train_mat_path = os.path.join(data_root, \"warm_train_matrix.npz\")\n",
                "if not os.path.exists(train_mat_path):\n",
                "    # Try alternative path if structure is different\n",
                "    train_mat_path = os.path.join(data_root, \"0.8\", \"warm_train_matrix.npz\") # 1-0.2=0.8\n",
                "\n",
                "print(f\"Loading train matrix from: {train_mat_path}\")\n",
                "train_mat = load_npz(train_mat_path)\n",
                "num_users, num_items = train_mat.shape\n",
                "print(f\"Num Users: {num_users}, Num Items: {num_items}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 54,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading checkpoint state dict from C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\model\\Beauty\\rec\\rec_model_0.1_300_0.2_stella_0.001.pth...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\vulam\\AppData\\Local\\Temp\\ipykernel_1052\\2451186131.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                        "  state_dict = torch.load(rec_model_path, map_location=device)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found item_src_embs in checkpoint: shape=(12101, 1024)\n"
                    ]
                }
            ],
            "source": [
                "# 2. Inspect Checkpoint to get Missing Dimensions (for stella embeddings)\n",
                "print(f\"Loading checkpoint state dict from {rec_model_path}...\")\n",
                "state_dict = torch.load(rec_model_path, map_location=device)\n",
                "\n",
                "# We need to know the shape of 'base_model.item_src_embs.weight' to create a dummy initialization\n",
                "item_src_embs_weight = state_dict.get('base_model.item_src_embs.weight')\n",
                "if item_src_embs_weight is not None:\n",
                "    stella_num_items, stella_emb_dim = item_src_embs_weight.shape\n",
                "    print(f\"Found item_src_embs in checkpoint: shape=({stella_num_items}, {stella_emb_dim})\")\n",
                "else:\n",
                "    print(\"WARNING: item_src_embs.weight not found in checkpoint!\")\n",
                "    stella_emb_dim = 1024 # Fallback guess or check error\n",
                "    stella_num_items = num_items"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Mock pickle.load to bypass missing file '/shared/user/embs/...'\n",
                "# The model init calls: pickle.load(open(..., 'rb'))\n",
                "# We will mock 'open' and 'pickle.load' specifically for this call.\n",
                "\n",
                "original_open = builtins.open\n",
                "original_pickle_load = pickle.load\n",
                "\n",
                "class MockFile:\n",
                "    def __enter__(self): return self\n",
                "    def __exit__(self, *args): pass\n",
                "    def read(self): return b\"\"\n",
                "\n",
                "def mocked_open(file, mode='r', *args, **kwargs):\n",
                "    if \"item_review_embeddings.pkl\" in str(file):\n",
                "        print(f\"Intercepted open for: {file}\")\n",
                "        return MockFile()\n",
                "    return original_open(file, mode, *args, **kwargs)\n",
                "\n",
                "def mocked_pickle_load(file_obj, *args, **kwargs):\n",
                "    if isinstance(file_obj, MockFile):\n",
                "        print(\"Intercepted pickle.load, returning dummy embeddings\")\n",
                "        # Return dummy numpy array with correct shape\n",
                "        return np.random.rand(stella_num_items, stella_emb_dim).astype(np.float32)\n",
                "    return original_pickle_load(file_obj, *args, **kwargs)\n",
                "\n",
                "# Activate mocks\n",
                "builtins.open = mocked_open\n",
                "pickle.load = mocked_pickle_load"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 56,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: b106f5ff-94d5-4c02-aeef-ce34d338bfef)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
                        "Retrying in 1s [Retry 1/5].\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading GPT2 from: c:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\model\\pretrained\\gpt2\n",
                        "Initializing MSEDynamicDPELLM4RecBaseModel...\n",
                        "Intercepted open for: embs\\Beauty_item_review_embeddings.pkl\n",
                        "Intercepted pickle.load, returning dummy embeddings\n",
                        "Intercepted open for: embs\\Beauty_item_review_embeddings.pkl\n",
                        "Intercepted pickle.load, returning dummy embeddings\n",
                        "Initializing DynamicCollaborativeGPTwithItemRecommendHead...\n",
                        "Mocks removed.\n"
                    ]
                }
            ],
            "source": [
                "# 4. Initialize Model components\n",
                "\n",
                "# Config\n",
                "config = GPT2Config.from_pretrained('gpt2')\n",
                "config.num_users = num_users\n",
                "config.num_items = num_items\n",
                "config.n_embd = config.hidden_size\n",
                "\n",
                "# LLM Model (GPT2)\n",
                "# Assuming internet access or cached model. \n",
                "# If offline, point to: os.path.join(root_dir, \"model\", \"pretrained\", \"gpt2\")\n",
                "llm_model_path = 'gpt2'\n",
                "local_gpt2 = os.path.join(root_dir, \"model\", \"pretrained\", \"gpt2\")\n",
                "if os.path.exists(local_gpt2):\n",
                "    llm_model_path = local_gpt2\n",
                "\n",
                "print(f\"Loading GPT2 from: {llm_model_path}\")\n",
                "LLMmodel = AutoModel.from_pretrained(llm_model_path)\n",
                "\n",
                "# Memory (Empty is fine for weight loading, but class needs it)\n",
                "memory = UserItemMemory()\n",
                "\n",
                "# Base Classifier (BERT)\n",
                "# Used for 'classifier' mode, but constructor might require it even for 'stella' if not careful.\n",
                "# The code in model.py init:\n",
                "# self.meta_logits_classifier = meta_logits_classifier.to(device) (unconditional assignment)\n",
                "# So we need to pass something valid or a mock.\n",
                "# Since we use 'stella', logic later: if item_logits_infer == 'stella': ...\n",
                "# It doesn't use meta_logits_classifier in 'stella' mode initialization logic (lines 201-206).\n",
                "# But it is assigned to self.meta_logits_classifier.\n",
                "# We will pass a dummy mock to avoid loading BERT.\n",
                "dummy_classifier = MagicMock()\n",
                "dummy_classifier.to.return_value = dummy_classifier\n",
                "\n",
                "print(\"Initializing MSEDynamicDPELLM4RecBaseModel...\")\n",
                "base_model = MSEDynamicDPELLM4RecBaseModel(\n",
                "    config,\n",
                "    LLMmodel,\n",
                "    memory,\n",
                "    meta_logits_tokenizer=None,\n",
                "    meta_logits_classifier=dummy_classifier,\n",
                "    device=device,\n",
                "    num_item_meta=num_meta,\n",
                "    item_logits_infer=item_logits_infer,\n",
                "    dataset_name=dataset\n",
                ")\n",
                "\n",
                "# Wrap in Recommend Head\n",
                "print(\"Initializing DynamicCollaborativeGPTwithItemRecommendHead...\")\n",
                "rec_model = DynamicCollaborativeGPTwithItemRecommendHead(config, base_model, device=device)\n",
                "\n",
                "# Restore original open/pickle\n",
                "builtins.open = original_open\n",
                "pickle.load = original_pickle_load\n",
                "print(\"Mocks removed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 57,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading state dict into model...\n",
                        "Model loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "# 5. Load Weights\n",
                "print(\"Loading state dict into model...\")\n",
                "rec_model.load_state_dict(state_dict, strict=False)\n",
                "rec_model.eval()\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "original\n"
                    ]
                }
            ],
            "source": [
                "print(rec_model.base_model.item_logits_infer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- Extracted Components ---\n",
                        "User Embeddings Shape: torch.Size([22363, 768])\n",
                        "Item Embeddings (Source/Stella) Shape: torch.Size([12101, 768])\n",
                        "Token-Meta Embeddings Shape: torch.Size([300, 768])\n",
                        "MLP Model: EmbeddingMapper(\n",
                        "  (model): Sequential(\n",
                        "    (0): Linear(in_features=1024, out_features=400, bias=True)\n",
                        "    (1): ReLU()\n",
                        "    (2): Dropout(p=0.1, inplace=False)\n",
                        "    (3): Linear(in_features=400, out_features=300, bias=True)\n",
                        "  )\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "# 6. Extract Components\n",
                "# - user embedding\n",
                "# - item embedding (base_model.item_src_embs)\n",
                "# - token-meta embedding (base_model.meta_item_embeddings)\n",
                "# - MLP to compute logits (base_model.item_emb_mapper)\n",
                "\n",
                "user_embeddings = rec_model.base_model.user_embeddings.weight.data\n",
                "# item_embeddings = rec_model.base_model.item_src_embs.weight.data\n",
                "item_embeddings = rec_model.base_model.item_embeddings.weight.data\n",
                "token_meta_embeddings = rec_model.base_model.meta_item_embeddings.weight.data\n",
                "mlp_model = rec_model.base_model.item_emb_mapper\n",
                "\n",
                "print(\"\\n--- Extracted Components ---\")\n",
                "print(f\"User Embeddings Shape: {user_embeddings.shape}\")\n",
                "print(f\"Item Embeddings (Source/Stella) Shape: {item_embeddings.shape}\")\n",
                "print(f\"Token-Meta Embeddings Shape: {token_meta_embeddings.shape}\")\n",
                "print(f\"MLP Model: {mlp_model}\")\n",
                "\n",
                "# Verify MLP Computation\n",
                "# logits = mlp(item_embedding) -> scaled to meta size\n",
                "# weighted_emb = probs(logits) @ meta_embeddings\n",
                "\n",
                "# with torch.no_grad():\n",
                "#     test_item_emb = item_embeddings[0].unsqueeze(0)\n",
                "#     logits = mlp_model(test_item_emb)\n",
                "#     print(f\"Test MLP Logits Shape: {logits.shape}\")  # Should be (1, num_meta)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 59,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor([[ 0.0685, -0.3258,  0.0877,  ..., -0.1668, -0.0320, -0.0072],\n",
                            "        [-0.3455, -0.1317,  0.1039,  ..., -0.1315,  0.1846, -0.4045],\n",
                            "        [-0.0949, -0.3196, -0.1555,  ...,  0.2724,  0.3822,  0.1360],\n",
                            "        ...,\n",
                            "        [ 0.2836,  0.1362, -0.0585,  ..., -0.1143,  0.2605,  0.1792],\n",
                            "        [-0.1187,  0.3221, -0.2905,  ..., -0.1308,  0.2045, -0.0372],\n",
                            "        [-0.2915,  0.2260, -0.3313,  ...,  0.0513, -0.1255,  0.1025]])"
                        ]
                    },
                    "execution_count": 59,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "token_meta_embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Content-Based Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Loading Evaluation Datasets ---\n",
                        "Loading Test Matrix: C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\cold_item_test_matrix.npz\n",
                        "Test Matrix Shape (Users x Items): (22363, 12101)\n",
                        "Loading Item Embeddings: C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\Beauty_item_review_embeddings_full.pkl\n",
                        "All Item Embeddings Shape: (12101, 1024)\n",
                        "Loading Warm Train Matrix: C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\warm_train_matrix.npz\n",
                        "Warm Train Matrix Shape: (22363, 12101)\n",
                        "\n",
                        "--- Computing User Embeddings and Evaluating ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\vulam\\AppData\\Local\\Temp\\ipykernel_1052\\3267154998.py:87: RuntimeWarning: invalid value encountered in divide\n",
                        "  batch_user_embs_norm = batch_user_embs / np.linalg.norm(batch_user_embs, axis=1, keepdims=True)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Final Content-Based Recall@20: 0.0331\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import scipy.sparse as sp\n",
                "\n",
                "# File Paths defined by User\n",
                "TEST_MATRIX_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\cold_item_test_matrix.npz\"\n",
                "ITEM_EMBS_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\Beauty_item_review_embeddings_full.pkl\"\n",
                "WARM_TRAIN_MATRIX_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\warm_train_matrix.npz\"\n",
                "\n",
                "print(\"--- Loading Evaluation Datasets ---\")\n",
                "\n",
                "# 1. Load Ground Truth (Cold Item Test Matrix)\n",
                "print(f\"Loading Test Matrix: {TEST_MATRIX_PATH}\")\n",
                "test_matrix = load_npz(TEST_MATRIX_PATH)\n",
                "print(f\"Test Matrix Shape (Users x Items): {test_matrix.shape}\")\n",
                "\n",
                "# 2. Load Item Embeddings\n",
                "print(f\"Loading Item Embeddings: {ITEM_EMBS_PATH}\")\n",
                "with open(ITEM_EMBS_PATH, \"rb\") as f:\n",
                "    # These are likely numpy arrays of shape (NumItems, EmbDim)\n",
                "    all_item_embeddings = pickle.load(f)\n",
                "print(f\"All Item Embeddings Shape: {all_item_embeddings.shape}\")\n",
                "\n",
                "# 3. Load User History (Warm Train Matrix)\n",
                "print(f\"Loading Warm Train Matrix: {WARM_TRAIN_MATRIX_PATH}\")\n",
                "warm_train_matrix = load_npz(WARM_TRAIN_MATRIX_PATH)\n",
                "print(f\"Warm Train Matrix Shape: {warm_train_matrix.shape}\")\n",
                "num_users_train, num_items_train = warm_train_matrix.shape\n",
                "\n",
                "# --- Evaluation Logic ---\n",
                "\n",
                "def calculate_recall_at_k(predictions, ground_truth, k=20):\n",
                "    \"\"\"\n",
                "    predictions: list of lists or 2D array, top K item indices for each user\n",
                "    ground_truth: list of lists or 2D sparse row, ground truth item indices for each user\n",
                "    \"\"\"\n",
                "    recalls = []\n",
                "    \n",
                "    # Convert sparse matrix to list of lists for easier iteration if needed, \n",
                "    # or iterate rows efficiently.\n",
                "    \n",
                "    for user_idx in range(len(predictions)):\n",
                "        pred_items = set(predictions[user_idx][:k])\n",
                "        \n",
                "        # Handle sparse matrix ground truth\n",
                "        if sp.issparse(ground_truth):\n",
                "            true_items = set(ground_truth[user_idx].indices)\n",
                "        else:\n",
                "            true_items = set(ground_truth[user_idx])\n",
                "        \n",
                "        if len(true_items) == 0:\n",
                "            continue\n",
                "            \n",
                "        hits = len(pred_items & true_items)\n",
                "        recalls.append(hits / len(true_items))\n",
                "        \n",
                "    return np.mean(recalls)\n",
                "\n",
                "print(\"\\n--- Computing User Embeddings and Evaluating ---\")\n",
                "\n",
                "# We only evaluate users who exist in the test matrix row-space.\n",
                "# Assuming the row indices in test_matrix correspond to the same users in warm_train_matrix.\n",
                "# (Usually indices are consistent 0..N-1).\n",
                "\n",
                "batch_size = 100\n",
                "num_test_users = test_matrix.shape[0]\n",
                "k = 20\n",
                "\n",
                "all_recalls = []\n",
                "\n",
                "norm_item_embeddings = all_item_embeddings / np.linalg.norm(all_item_embeddings, axis=1, keepdims=True)\n",
                "norm_item_embeddings = np.nan_to_num(norm_item_embeddings) # Handle potential zero vectors\n",
                "\n",
                "for start_idx in range(0, num_test_users, batch_size):\n",
                "    end_idx = min(start_idx + batch_size, num_test_users)\n",
                "    user_indices = np.arange(start_idx, end_idx)\n",
                "\n",
                "    user_history_batch = warm_train_matrix[user_indices]\n",
                "    \n",
                "    batch_user_embeddings = []\n",
                "    batch_user_sums = user_history_batch @ all_item_embeddings\n",
                "    \n",
                "    user_counts = np.array(user_history_batch.sum(axis=1)).flatten()\n",
                "    user_counts[user_counts == 0] = 1 # Avoid division by zero\n",
                "    \n",
                "    batch_user_embs = batch_user_sums / user_counts[:, np.newaxis]\n",
                "    \n",
                "    batch_user_embs_norm = batch_user_embs / np.linalg.norm(batch_user_embs, axis=1, keepdims=True)\n",
                "    batch_user_embs_norm = np.nan_to_num(batch_user_embs_norm)\n",
                "    \n",
                "    scores = batch_user_embs_norm @ norm_item_embeddings.T\n",
                "\n",
                "    top_k_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
                "    batch_recalls = []\n",
                "    test_interactions_batch = test_matrix[user_indices]\n",
                "    \n",
                "    for i in range(len(user_indices)):\n",
                "        pred_indices = top_k_indices[i]\n",
                "        true_indices = test_interactions_batch[i].indices\n",
                "        \n",
                "        if len(true_indices) > 0:\n",
                "            hits = np.intersect1d(pred_indices, true_indices).size\n",
                "            batch_recalls.append(hits / len(true_indices))\n",
                "            \n",
                "    all_recalls.extend(batch_recalls)\n",
                "\n",
                "final_recall_20 = np.mean(all_recalls)\n",
                "print(f\"\\nFinal Content-Based Recall@20: {final_recall_20:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refined Content-Based Evaluation (MLP + Meta-Item Embeddings)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Computing Refined Item Embeddings ---\n",
                        "Item Logits Shape: torch.Size([12101, 300])\n",
                        "Refined Item Embeddings Shape: torch.Size([12101, 768])\n",
                        "--- Evaluating with Refined Embeddings ---\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\vulam\\AppData\\Local\\Temp\\ipykernel_1052\\3426589990.py:62: RuntimeWarning: invalid value encountered in divide\n",
                        "  batch_user_embs_norm = batch_user_embs / np.linalg.norm(batch_user_embs, axis=1, keepdims=True)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Final Refined Content-Based Recall@20: 0.0383\n"
                    ]
                }
            ],
            "source": [
                "print(\"--- Computing Refined Item Embeddings ---\")\n",
                "\n",
                "# 1. Convert Raw Embeddings to Tensor\n",
                "raw_item_embs_tensor = torch.tensor(all_item_embeddings, dtype=torch.float32).to(device)\n",
                "\n",
                "# 2. Calculate New Content Embeddings via Model Pipeline\n",
                "\n",
                "# Ensure models are in evaluation mode\n",
                "mlp_model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    # A. Pass through MLP to get Logits (n_items, n_meta)\n",
                "    # mlp_model (EmbeddingMapper) is likely (EmbDim -> ... -> NumMeta)\n",
                "    item_logits = mlp_model(raw_item_embs_tensor)\n",
                "    print(f\"Item Logits Shape: {item_logits.shape}\")\n",
                "    \n",
                "    # B. Apply Softmax to get probabilities (Weights)\n",
                "    # Consistent with model.py get_probs(self, logits) with prob_norm='softmax'\n",
                "    item_probs = torch.nn.functional.softmax(item_logits, dim=-1)\n",
                "    \n",
                "    # C. Multiply with Meta-Item Embeddings\n",
                "    # token_meta_embeddings is a Tensor (NumMeta, EmbDim)\n",
                "    # weighted_item_embeddings = item_probs @ meta_item_embeddings\n",
                "    # Shape: (NumItems, NumMeta) @ (NumMeta, EmbDim) -> (NumItems, EmbDim)\n",
                "    if not isinstance(token_meta_embeddings, torch.Tensor):\n",
                "        token_meta_embeddings = torch.tensor(token_meta_embeddings).to(device)\n",
                "    \n",
                "    token_meta_embeddings = token_meta_embeddings.cuda()\n",
                "    refined_item_embeddings = torch.matmul(item_probs, token_meta_embeddings)\n",
                "    print(f\"Refined Item Embeddings Shape: {refined_item_embeddings.shape}\")\n",
                "\n",
                "# Convert back to Numpy for evaluation loop\n",
                "refined_item_embeddings_np = refined_item_embeddings.cpu().numpy()\n",
                "\n",
                "print(\"--- Evaluating with Refined Embeddings ---\")\n",
                "\n",
                "# Reuse Evaluation Loop logic, but with refined_item_embeddings_np\n",
                "\n",
                "all_recalls_refined = []\n",
                "\n",
                "# Normalized Refined Item Embeddings\n",
                "norm_refined_item_embeddings = refined_item_embeddings_np / np.linalg.norm(refined_item_embeddings_np, axis=1, keepdims=True)\n",
                "norm_refined_item_embeddings = np.nan_to_num(norm_refined_item_embeddings)\n",
                "\n",
                "for start_idx in range(0, num_test_users, batch_size):\n",
                "    end_idx = min(start_idx + batch_size, num_test_users)\n",
                "    user_indices = np.arange(start_idx, end_idx)\n",
                "    \n",
                "    # Input User History is still the same warm interactions\n",
                "    user_history_batch = warm_train_matrix[user_indices]\n",
                "    \n",
                "    # Calculate User Embeddings using REFINED item embeddings\n",
                "    # BatchUserSums = History @ RefinedItemMatrix\n",
                "    batch_user_sums = user_history_batch @ refined_item_embeddings_np\n",
                "    \n",
                "    user_counts = np.array(user_history_batch.sum(axis=1)).flatten()\n",
                "    user_counts[user_counts == 0] = 1\n",
                "    \n",
                "    batch_user_embs = batch_user_sums / user_counts[:, np.newaxis]\n",
                "    \n",
                "    # Norm\n",
                "    batch_user_embs_norm = batch_user_embs / np.linalg.norm(batch_user_embs, axis=1, keepdims=True)\n",
                "    batch_user_embs_norm = np.nan_to_num(batch_user_embs_norm)\n",
                "    \n",
                "    # Scores: Users @ RefinedItems.T\n",
                "    scores = batch_user_embs_norm @ norm_refined_item_embeddings.T\n",
                "    \n",
                "    # Top K\n",
                "    top_k_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
                "    \n",
                "    # Evaluate against SAME ground truth\n",
                "    batch_recalls = []\n",
                "    test_interactions_batch = test_matrix[user_indices]\n",
                "    \n",
                "    for i in range(len(user_indices)):\n",
                "        pred_indices = top_k_indices[i]\n",
                "        true_indices = test_interactions_batch[i].indices\n",
                "        \n",
                "        if len(true_indices) > 0:\n",
                "            hits = np.intersect1d(pred_indices, true_indices).size\n",
                "            batch_recalls.append(hits / len(true_indices))\n",
                "            \n",
                "    all_recalls_refined.extend(batch_recalls)\n",
                "\n",
                "final_recall_20_refined = np.mean(all_recalls_refined)\n",
                "print(f\"\\nFinal Refined Content-Based Recall@20: {final_recall_20_refined:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Warm Test Data using Model Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Evaluating Warm Test Data with Model Learned Embeddings ---\n",
                        "Loading Warm Test Matrix: C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\warm_test_matrix.npz\n",
                        "Warm Test Matrix Shape: (22363, 12101)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\vulam\\AppData\\Local\\Temp\\ipykernel_1052\\4072109024.py:37: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
                        "  scores = batch_user_embs @ norm_refined_item_embeddings.T\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Final Warm Test Recall@20 (Model Embeddings): 0.0081\n"
                    ]
                }
            ],
            "source": [
                "WARM_TEST_MATRIX_PATH = r\"C:\\Users\\vulam\\Downloads\\Master_papers\\code\\cold_item\\MI4Rec\\dataset\\beauty\\warm_test_matrix.npz\"\n",
                "\n",
                "print(\"--- Evaluating Warm Test Data with Model Learned Embeddings ---\")\n",
                "\n",
                "# 1. Load Warm Test Matrix\n",
                "print(f\"Loading Warm Test Matrix: {WARM_TEST_MATRIX_PATH}\")\n",
                "warm_test_matrix = load_npz(WARM_TEST_MATRIX_PATH)\n",
                "print(f\"Warm Test Matrix Shape: {warm_test_matrix.shape}\")\n",
                "\n",
                "# 2. Use Model Embeddings directly\n",
                "# user_embeddings: Learned User Embeddings from model (NumUsers, EmbDim) (Tensor)\n",
                "# refined_item_embeddings_np: Refined Item Embeddings (NumItems, EmbDim) (Numpy)\n",
                "\n",
                "if isinstance(user_embeddings, torch.Tensor):\n",
                "    user_embeddings_np = user_embeddings.cpu().numpy()\n",
                "else:\n",
                "    user_embeddings_np = user_embeddings\n",
                "\n",
                "# Normalize for Cosine Similarity\n",
                "user_embeddings_norm = user_embeddings\n",
                "\n",
                "# norm_refined_item_embeddings = refined_item_embeddings_np / np.linalg.norm(refined_item_embeddings_np, axis=1, keepdims=True)\n",
                "# norm_refined_item_embeddings = np.nan_to_num(norm_refined_item_embeddings)\n",
                "item_embeddings = item_embeddings.cpu().numpy()\n",
                "norm_refined_item_embeddings = item_embeddings\n",
                "all_warm_recalls = []\n",
                "num_users = warm_test_matrix.shape[0]\n",
                "\n",
                "for start_idx in range(0, num_users, batch_size):\n",
                "    end_idx = min(start_idx + batch_size, num_users)\n",
                "    user_indices = np.arange(start_idx, end_idx)\n",
                "    \n",
                "    # Batch User Embeddings\n",
                "    batch_user_embs = user_embeddings_norm[user_indices]\n",
                "    \n",
                "    # Calculate Scores\n",
                "    scores = batch_user_embs @ norm_refined_item_embeddings.T\n",
                "    \n",
                "    # Top K\n",
                "    top_k_indices = np.argpartition(-scores, k, axis=1)[:, :k]\n",
                "    \n",
                "    # Evaluation Match\n",
                "    warm_test_batch = warm_test_matrix[user_indices]\n",
                "    \n",
                "    batch_recalls = []\n",
                "    for i in range(len(user_indices)):\n",
                "        pred_indices = top_k_indices[i]\n",
                "        true_indices = warm_test_batch[i].indices\n",
                "        \n",
                "        if len(true_indices) > 0:\n",
                "            hits = np.intersect1d(pred_indices, true_indices).size\n",
                "            batch_recalls.append(hits / len(true_indices))\n",
                "            \n",
                "    all_warm_recalls.extend(batch_recalls)\n",
                "\n",
                "final_warm_recall_20 = np.mean(all_warm_recalls)\n",
                "print(f\"\\nFinal Warm Test Recall@20 (Model Embeddings): {final_warm_recall_20:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
